{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import PreTrainedModel, AutoModel, LlamaModel, AutoTokenizer\n",
    "from transformers.file_utils import ModelOutput\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "from tevatron.arguments import ModelArguments, DataArguments, \\\n",
    "    TevatronTrainingArguments as TrainingArguments\n",
    "from data import HFTrainDataset, TrainDataset, TrainCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderOutput(ModelOutput):\n",
    "    q_reps: Optional[Tensor] = None\n",
    "    p_reps: Optional[Tensor] = None\n",
    "    loss: Optional[Tensor] = None\n",
    "    scores: Optional[Tensor] = None\n",
    "\n",
    "\n",
    "class EncoderModel(torch.nn.Module):\n",
    "    def __init__(self, lm_q: PreTrainedModel, lm_p: PreTrainedModel, pooler: nn.Module=None):\n",
    "        super(EncoderModel, self).__init__()\n",
    "\n",
    "        self.lm_q = lm_q\n",
    "        self.lm_p = lm_p\n",
    "        self.pooler = pooler\n",
    "        self.cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "    def forward(self, query: Dict[str, Tensor] = None, passage: Dict[str, Tensor] = None):\n",
    "        q_reps = self.encode_query(query)\n",
    "        p_reps = self.encode_passage(passage)\n",
    "        loss = None\n",
    "        scores = None\n",
    "\n",
    "        # traingin\n",
    "        if self.training:\n",
    "            scores = self.compute_similarity(q_reps, p_reps)\n",
    "\n",
    "            # 類似度スコアのテンソルをクエリごとの類似度スコアの行列に整形し直す\n",
    "            # (クエリの数, パッセージの数 / クエリの数)\n",
    "            scores = scores.view(q_reps.size(0), -1)\n",
    "\n",
    "            # クエリごとの正解ラベルを整形し直す\n",
    "            # クエリに対して関連するパッセージの分だけインデックスを調整\n",
    "            taregt = torch.arange(scores.size(0), device=scores.device, dtype=torch.long)\n",
    "            target = target * (p_reps.size(0) // query_size)\n",
    "\n",
    "            loss = self.compute_loss(scores, target)\n",
    "\n",
    "        return EncoderOutput(\n",
    "            loss=loss,\n",
    "            scores=scores,\n",
    "            q_reps=q_reps,\n",
    "            p_reps=p_reps,\n",
    "        )\n",
    "\n",
    "    def encode_passage(self, passage):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode_query(self, query):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_similarity(self, q_reps, p_reps):\n",
    "        return torch.matmul(q_reps, p_reps.transpose(0, 1))\n",
    "\n",
    "    def comupte_loss(self, scores, target):\n",
    "        raise self.cross_entropy(scores, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepLLaMA(EncoderModel):\n",
    "    def __init__(self, lm_q: PreTrainedModel, lm_p: PreTrainedModel, pooler: nn.Module=None):\n",
    "        super(RepLLaMA, self).__init__(lm_q, lm_p, pooler)\n",
    "\n",
    "    def encode_passage(self, passage):\n",
    "        if passage is None:\n",
    "            return None\n",
    "\n",
    "        passage_output = self.lm_p(**passage, output_hidden_states=True)\n",
    "        p_hidden = passage_output.hidden_states[-1]\n",
    "        attention_mask = passage[\"attention_mask\"]\n",
    "\n",
    "        # paddingではない最後のトークンに対応する埋め込み表現を取得\n",
    "        ## 行ごとにpaddingされていない部分=実際のトークンの数を集計\n",
    "        sequrnce_lengths = attention_mask.sum(dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "\n",
    "        ## バッチ内の各系列の隠れ層の出力\n",
    "        ### p_hidden: (batch_size, seq_len, hidden_size)\n",
    "        p_reps = p_hidden[torch.arange(p_hidden.size(0)), last_token_indices]\n",
    "        p_reps = nn.functional.normalize(p_reps, p=2, dim=-1)\n",
    "        return p_reps\n",
    "\n",
    "    def encode_query(self, query):\n",
    "        if query is None:\n",
    "            return None\n",
    "\n",
    "        query_output = self.lm_q(**query, output_hidden_states=True)\n",
    "        q_hidden = query_output.hidden_states[-1]\n",
    "        attention_mask = query[\"attention_mask\"]\n",
    "\n",
    "        # paddingではない最後のトークンに対応する埋め込み表現を取得\n",
    "        ## 行ごとにpaddingされていない部分=実際のトークンの数を集計\n",
    "        sequrnce_lengths = attention_mask.sum(dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "\n",
    "        ## バッチ内の各系列の隠れ層の出力\n",
    "        ### q_hidden: (batch_size, seq_len, hidden_size)\n",
    "        q_reps = q_hidden[torch.arange(q_hidden.size(0)), last_token_indices]\n",
    "        q_reps = nn.functional.normalize(q_reps, p=2, dim=-1)\n",
    "        return q_reps\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.lm_q.base_model.gradient_checkpointing_enable()\n",
    "\n",
    "    # いる？？\n",
    "    @staticmethod\n",
    "    def build_peft_model(peft_model_name: str):\n",
    "        config = LoraConfig.from_pretrained(peft_model_name)\n",
    "        config.inference_mode = False\n",
    "        base_model = LlamaModel.from_pretrained(config.base_model_name_or_path)\n",
    "        model = get_peft_model(base_model, config)\n",
    "        model.print_trainable_parameters()\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, model_config, train_config, **hf_kwargs):\n",
    "        base_model = LlamaModel.from_pretrained(model_config.model_name_or_path, **hf_kwargs)\n",
    "\n",
    "        if train_config.gradient_checkpointing:\n",
    "            base_model.gradient_checkpointing_enable()\n",
    "\n",
    "        if base_model.config.pad_token_id is None:\n",
    "            base_model.config.pad_token_id = 0\n",
    "\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            base_model_name_or_path=model_config.model_name_or_path,\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"down_proj\", \"up_proj\", \"gate_proj\"],\n",
    "            inference_mode=False\n",
    "        )\n",
    "\n",
    "        hf_model = get_peft_model(base_model, peft_config)\n",
    "        model = cls(\n",
    "            lm_q=hf_model,\n",
    "            lm_p=hf_model,\n",
    "            pooler=None,  \n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        self.lm_q.save_pretrained(output_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    dataset_name=\"Tevatron/msmarco-passage\",\n",
    "    train_n_passages=16,\n",
    "    q_max_len=32,\n",
    "    p_max_len=128,\n",
    "    dataset_proc_num=32\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_repllama\",\n",
    "    save_steps=20,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=10,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model = RepLLaMA.build(\n",
    "    model_args,\n",
    "    training_args,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
